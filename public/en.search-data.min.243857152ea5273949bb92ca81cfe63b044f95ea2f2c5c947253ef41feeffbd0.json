[{"id":0,"href":"/docs/research/","title":"ğŸ§ª Research","section":"Docs","content":"My long-term goal is to make AI more responsible\u0026mdash;accessible, sustainable, and righteous.\nCurrently, I am focusing on various facets of Efficient ML. Together with EffLers, I am striving to enable ML without giant-scale models/data/compute by innovating relevant theories, algorithms, and systems. Some recent research questions that I am specifically interested in are:\nHow (and to what extent) can we reduce the data-level redundancy for efficient training? How can we replace the noise inherent in ML with a compute-efficient one? Publications. See the group webpage.\n"},{"id":1,"href":"/docs/work/","title":"ğŸ§‘â€ğŸ’» Employment","section":"Docs","content":" Current # Assistant Professor EE@POSTECH (adj. AI, DST)\n2022.03\u0026ndash;Present Adjunct Professor, iCreate@Yonsei\n2022.09\u0026ndash;Present Visiting Faculty Researcher, Google\n2023.09\u0026ndash;Present Previous # Postdoctoral Researcher, EE@KAIST (ì „ë¬¸ì—°êµ¬ìš”ì›)\nHost: Jinwoo Shin\n2019.03\u0026ndash;2022.03 ASAN Fellow, Center for Data Analysis @ The Heritage Foundation\nHost: Salim Furth, James Sherk\n2013.05\u0026ndash;2013.07 "},{"id":2,"href":"/docs/education/","title":"ğŸ“ Education","section":"Docs","content":" Ph.D. in Electrical and Computer Engineering (2019) # Alma Mater. University of Illinois at Urbana-Champaign\nAdvisor. Maxim Raginsky\nThesis. \u0026ldquo;Robustness and Generalization Guarantees for Statistical Learning of Generative Models\u0026rdquo;\nM.S. in Electrical and Computer Engineering (2015) # Alma Mater. University of Illinois at Urbana-Champaign\nAdvisor. Maxim Raginsky\nThesis. \u0026ldquo;MMSE estimation from Quantized Observations in the Nonasymptotic Regime\u0026rdquo;\nB.S. in Electrical Engineering, Management Science (2013) # Alma Mater. Korea Advanced Institute of Science and Technology\nAdvisor. Yung Yi\ndouble major, summa cum laude\n"},{"id":3,"href":"/docs/teaching/","title":"ğŸ‘¨â€ğŸ« Teaching","section":"Docs","content":" 2023 Fall # EECE454 Introduction to Machine Learning Systems\n2023 Spring # EECE695D Deep Learning Theory\n2022 Fall # EECE695D-01 Efficient Machine Learning Systems\n(EduTech Teaching Excellence Award)\nPast Courses # As a TA, I have taught:\nFA17 TA, ECE563@UIUC Information theory. SP17 TA, ECE498@UIUC Introduction to stochastic systems. FA15 TA, ECE598@UIUC Statistical learning theory. "},{"id":4,"href":"/docs/teaching/fall23/","title":"23F - Intro to ML Sys","section":"ğŸ‘¨â€ğŸ« Teaching","content":" EECE454: Introduction to ML Systems (Fall 2023) # Team # Instructor. Jaeho Lee ì´ì¬í˜¸\nfirstname.lastname@postech.ac.kr TA. Minkyu Kim ê¹€ë¯¼ê·œ\nfirstname.lastname@postech.ac.kr Location \u0026amp; Time # Class. Mondays/Wednesdays, 09:30AM\u0026ndash;10:45AM @ 106 LG Hall Office Hr. Mondays, 04:00PM\u0026ndash;05:00PM @ 407 Eng Bldg 2 Schedule (tentative) # W1. Introduction / Basic Linear Algebra\n(9/4, 9/6) W2. Basic Probability / Simple Models\n(9/11, 9/13) W3. Simple Models / Support Vector Machines\n(9/18, 9/20) W4. Kernel SVM / K-Means Clustering\n(9/25, 9/27) W5. Gaussian Mixture Models\n(10/2, 10/4) W6. Decision Tree\n(10/9, 10/11) W7. Dimensionality Reduction\n(10/16, 10/18) W8. Mid-Term\n(10/23, 10/25) W9. Deep Learning Basics / Convolutional Networks\n(10/30, 11/1) W10. Backprop / Neural Network Training\n(11/6, 11/8) W11. Neural Network Training / -\n(11/13, 11/15) W12. Deep ConvNets / Generative Models\n(11/20, 11/22) W13. Generative Models / Language Models\n(11/27, 11/29) W14. Language Models / Multimodal Learning\n(12/4, 12/6) W15. Efficient ML / Deep Learning Theory\n(12/11, 12/13) W16. Final Project Presentation\n(12/18, 12/20) "},{"id":5,"href":"/docs/teaching/spring24/","title":"24S - (under construction)","section":"ğŸ‘¨â€ğŸ« Teaching","content":" EECE695E: Efficient ML Systems (Spring 2024) # Team # Instructor. Jaeho Lee ì´ì¬í˜¸\nfirstname.lastname@postech.ac.kr TA. (forthcoming) Location \u0026amp; Time # Class. (forthcoming) Office Hr. (forthcoming) What we\u0026rsquo;ll cover (hopefully) # Deep Learning Basics Architectures and Counting FLOPs Hardware bits Making models smaller Quantization Pruning \u0026amp; Sparsity Neural Architecture Search How to utilize the experience of other models Transfer Learning \u0026amp; Distillation Meta-Learning \u0026amp; Test-time Training Model Merging \u0026amp; Stitching Hyperparameter Transfer Prompt Tuning Organizing Large-scale Learning Parallelism \u0026amp; Pipelining Federated Learning Data Efficiency Data Compression Dataset Distillation \u0026amp; Condensation / SeiT Tips \u0026amp; Tricks for Large Transformers KV cache / FlashAttention / PagedAttention Speculative Decoding / Medusa Continuous Batching Schedule (tentative) # (forthcoming)\n"},{"id":6,"href":"/docs/misc/","title":"ï¹– Misc.","section":"Docs","content":" Talks (selected) # Tutorial, KICS Fall Conference, Gyeongju (2023.11.22)\n\u0026ldquo;Compressing Giant Neural Networks\u0026rdquo; Talk, Optimization \u0026amp; Machine Learning Workshop, UNIST (2023.08.24)\n\u0026ldquo;Problems that Arise in the Optimization of Neural Fields\u0026rdquo; Tutorial, KAIA Summer Conference, Yeosu (2023.07.17)\n\u0026ldquo;Recent Trends in Neural Data Compression: From Generative Models to Neural Fields\u0026rdquo; Tutorial, Korea SatCom Forum, Seoul (2023.05.31)\n\u0026ldquo;Recent Trends in Large Language Models\u0026rdquo; Miscellaneous Writings # Greece: Austerity Doesn\u0026rsquo;t Involve Public-Sector Layoffs\nJaeho Lee\nThe Daily Signal, 2023 (The Orange County Register, 2023)\në‚˜ë§Œì˜ ëª©í‘œ, ë‚˜ë§Œì˜ ê¸°ì¤€\nì´ì¬í˜¸\në™ì•„ì¼ë³´: ë‚´ê°€ ë§Œë‚œ ëª…ë¬¸ì¥, 2022\n"}]