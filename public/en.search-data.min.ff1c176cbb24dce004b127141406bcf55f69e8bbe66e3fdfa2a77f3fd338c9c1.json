[{"id":0,"href":"/docs/research/","title":"Research","section":"Docs","content":"Long-term Goal. Making AI more responsible: accessible, sustainable, and righteous.\nCurrent Focus. Various facets of effcient machine learning.\nIn particular, I try to answer these questions.\nGiven a fixed resource for training and inference, what is the best machine intelligence we can get? How can we effectively utilize the data-level redundancy for efficient training? How can we replace the noise, inherent in ML, with a compute-efficient option? Papers. See the group webpage.\n"},{"id":1,"href":"/docs/teaching/","title":"Teaching","section":"Docs","content":" 2023 Fall # EECE454 Introduction to Machine Learning Systems\n2023 Spring # EECE695D Deep Learning Theory (comments from students)\n2022 Fall # EECE695D-01 Efficient Machine Learning Systems (comments from students)\n(EduTech Teaching Excellence Award)\nPast Courses # As a TA, I have taught:\nFA17 TA, ECE563@UIUC Information theory. SP17 TA, ECE498@UIUC Introduction to stochastic systems. FA15 TA, ECE598@UIUC Statistical learning theory. "},{"id":2,"href":"/docs/teaching/spring24/","title":"24S - Efficient ML","section":"Teaching","content":" EECE695E: Efficient ML Systems (Spring 2024) # Team # Instructor. Jaeho Lee 이재호\nfirstname.lastname@postech.ac.kr TA. Hagyeong Lee 이하경\nfirstnamelastname@postech.ac.kr Location \u0026amp; Time # Class. Mondays \u0026amp; Wednesdays, 11:00AM\u0026ndash;12:15PM, PIAI 122. Office Hr. Mondays 5:00PM\u0026ndash;6:00PM, Eng. Building #407 (+ by appointment). Schedule (tentative) # W1. Deep Learning Recap (2/19) Introduction to Efficient ML \u0026amp; Logistics (2/21) Computations of DL W2. Sparsity (2/26) Algorithmic aspects of pruning (2/28) System aspects of pruning W3. Quantization + Special Lecture (3/4) Basic ideas of quantization (3/6 -\u0026gt; 3/8, Fri) Special Lecture by Tae-ho Kim @ Nota W4. Quantization + Distillation (3/11) PTQ and QAT (3/13) Distillation W5. Neural Architecture Search (3/18) Search Space, Search Strategy (pt 1) (3/20) Search Strategy (pt 2), Performance Evaluation W6. Adaptation (3/25) Continual Learning (3/27) Meta-Learning \u0026amp; Test-time Training W7. Parallelism (4/1) Data and Model Parallelism (4/3) Advanced Topics W8. (Mid-term week \u0026amp; Election Day) (4/8, 4/10) W9. Model Merging \u0026amp; Editing (4/15) Model Merging (4/17) Model Editing W10. Data Efficiency (4/22) Dataset Compression (4/24) Data Compression W11. Topics on LLM - 1 (4/29) Transformers \u0026amp; LLM Basics (5/1) Efficient Attention W12. Topics on LLM - 2 (5/6) Parameter-Efficient Fine-Tuning (5/8) Parameter-Efficient Fine-Tuning W13. Topics on LLM - 3 (5/13) Decoding Strategies (5/15) Buddha\u0026rsquo;s Day W14. Topics on Diffusion Models (5/20) LLM Compression (5/22) Topics in long sequence handling W15. Presentations Week - 1 (5/27, 5/29) W16. Presentations Week - 2 (6/3, 6/5) Recommended Materials # Blog Posts LLM inference performance engineering by Databricks Videos State-space models tutorial by Sasha Rush Trends in deep learning hardware by Bill Dally Related Courses Efficient deep learning systems at HSE university and Yandex Machine learning compilation at CMU TinyML and efficient deep learning at MIT Machine learning hardware and systems at Cornell Advances in Foundation Models at Stanford Books and surveys Algorithms for modern hardware by Sergey Slotin Efficient deep learning book by Menghani and Singh Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities by Bartoldson et al. "},{"id":3,"href":"/docs/teaching/fall23/","title":"23F - Intro to ML","section":"Teaching","content":" EECE454: Introduction to ML Systems (Fall 2023) # Team # Instructor. Jaeho Lee 이재호\nfirstname.lastname@postech.ac.kr TA. Minkyu Kim 김민규\nfirstname.lastname@postech.ac.kr Location \u0026amp; Time # Class. Mondays/Wednesdays, 09:30AM\u0026ndash;10:45AM @ 106 LG Hall Office Hr. Mondays, 04:00PM\u0026ndash;05:00PM @ 407 Eng Bldg 2 Schedule (tentative) # W1. Introduction / Basic Linear Algebra\n(9/4, 9/6) W2. Basic Probability / Simple Models\n(9/11, 9/13) W3. Simple Models / Support Vector Machines\n(9/18, 9/20) W4. Kernel SVM / K-Means Clustering\n(9/25, 9/27) W5. Gaussian Mixture Models\n(10/2, 10/4) W6. Decision Tree\n(10/9, 10/11) W7. Dimensionality Reduction\n(10/16, 10/18) W8. Mid-Term\n(10/23, 10/25) W9. Deep Learning Basics / Convolutional Networks\n(10/30, 11/1) W10. Backprop / Neural Network Training\n(11/6, 11/8) W11. Neural Network Training / -\n(11/13, 11/15) W12. Deep ConvNets / Generative Models\n(11/20, 11/22) W13. Generative Models / Language Models\n(11/27, 11/29) W14. Language Models / Multimodal Learning\n(12/4, 12/6) W15. Efficient ML / Deep Learning Theory\n(12/11, 12/13) W16. Final Project Presentation\n(12/18, 12/20) "},{"id":4,"href":"/docs/me/","title":"About me","section":"Docs","content":" Appointments # Current # Assistant Professor EE@POSTECH (adj. AI, DST)\n2022.03\u0026ndash;Present Adjunct Professor, iCreate@Yonsei\n2022.09\u0026ndash;Present Visiting Faculty Researcher, Google\n2023.09\u0026ndash;Present Previous # Postdoctoral Researcher, EE@KAIST (전문연구요원)\nHost: Jinwoo Shin\n2019.03\u0026ndash;2022.03 ASAN Fellow, Center for Data Analysis @ The Heritage Foundation\nHost: Salim Furth, James Sherk\n2013.05\u0026ndash;2013.07 Education # Ph.D. (2019). Electrical and Computer Engineering\nUniversity of Illinois at Urbana-Champaign\nAdvisor: Maxim Raginsky\nThesis: \u0026ldquo;Robustness and Generalization Guarantees for Statistical Learning of Generative Models\u0026rdquo;\nM.S. (2015). Electrical and Computer Engineering\nUniversity of Illinois at Urbana-Champaign\nAdvisor: Maxim Raginsky\nThesis: \u0026ldquo;MMSE estimation from Quantized Observations in the Nonasymptotic Regime\u0026rdquo;\nB.S. (2013). Electrical Engineering, Management Science (double major)\nKorea Advanced Institute of Science and Technology\nHonor: Summa cum laude\nTalks (selected) # Tutorial, KICS Fall Conference, Gyeongju (2023.11.22)\n\u0026ldquo;Compressing Giant Neural Networks\u0026rdquo; Talk, Optimization \u0026amp; Machine Learning Workshop, UNIST (2023.08.24)\n\u0026ldquo;Problems that Arise in the Optimization of Neural Fields\u0026rdquo; Tutorial, KAIA Summer Conference, Yeosu (2023.07.17)\n\u0026ldquo;Recent Trends in Neural Data Compression: From Generative Models to Neural Fields\u0026rdquo; Tutorial, Korea SatCom Forum, Seoul (2023.05.31)\n\u0026ldquo;Recent Trends in Large Language Models\u0026rdquo; Miscellaneous Writings # Greece: Austerity Doesn\u0026rsquo;t Involve Public-Sector Layoffs\nJaeho Lee\nThe Daily Signal, 2013 (The Orange County Register, 2013)\n나만의 목표, 나만의 기준\n이재호\n동아일보: 내가 만난 명문장, 2022\n어떤 연구를 할지 모르겠어요\n이재호\n포항공대신문: 노벨동산, 2023\n"},{"id":5,"href":"/docs/teaching/comments/_dlt_2023/","title":"Comments from DLT (2023 Spring)","section":"Teaching","content":"Here are some comments from the students\u0026mdash;I take this comments by heart.\n교수님께서 수업 준비를 충실히 하신 것이 느껴집니다. 한 학기 동안 유익한 내용을 배울 수 있는 좋은 기회였습니다. 좋은 강의 감사합니다~ 수업 내용이 어려워 가끔 따라가기 어려웠음. 과제 내용은 수업과 관련이 있지만, 개인적으로는 강의 슬라이드와 수업 교재만 보고 과제를 풀 수 없었던 경우가 많았기 때문에, 다른 교재들을 참고하고 따로 공부하는 시간을 많이 가질 수 밖에 없었음. 이해만 잘 하고 시간 투자를 좀 더 한다면 이론적으로 많은 도움을 받을 수 있을 것 같음. ppt가 좀 더 highlevel에서의 흐름을 잘 이해할 수 있도록 구성되면 좋겠습니다. 식 전개를 정신 없이 따라적다가 정작 어떤 식을 전개했는지 그 의미를 잘 이해하지 못하고 지나가버리는 경우가 많아서 아쉬웠습니다. Jaeho: Thank you for the great suggestion. I will try to add some \u0026ldquo;overview\u0026rdquo; slides in the beginning and the end of all lectures. 좋은 강의를 해주셔서 감사합니다. 연구에 analyzing을 사용할 수 있는 기초를 만들어 주셨습니다! 교수님께서 수업 준비에 신경쓰신것이 느껴지는 수업입니다. 교수님께서 어려운 분야에 대해 이해하기 쉽도록 과목을 구성하시고 설명을 자세히 해주셨기 때문에 좋은 과목이었다고 생각합니다. 다만, 개인적으로는 분야가 어렵고 제가 익숙하지 않은 점 때문에 이해가 어려운 부분이 많았던 부분이 아쉬운 점이었던 것 같습니다. 딥러닝 뒤에 있는 이론에 대해 알게 되어 정말 유익했습니다. 정말 흥미로웠고 이런 수업이 많이 생겼으면 좋겠습니다 아주 흥미롭지만 어려운 내용을 최대한 잘 풀어내어 설명하십니다. 수업 로드 또한 학생을 잘 배려해 주십니다. 최고의 교수님이십니다. The professor has a very good command in English. The lectures are challenging, but well designed. I do recommend this course for people who are interested in ML/DL or just the theoretical aspect of things. Being able to implement ML/DL algorithm is nice, but having a strong foundation in theoretical background do give you an edge, not only in research but also when working in the industry. 형식적인 과목들을 들을때는 시간도 아깝고 현타가 왔는데, 이 수업은 매우 유익했고 이러기 위해 대학원에 왔지라는 만족감도 들었습니다. 목소리는 잘 안들리고 수업시간에 이야기 해준 내용을 이해가 잘 안되서 메일로 질문하면 수업에서 다 한 얘기라고, 수업을 안들었냐고 말하면서 결국 질문에 대한 답변을 주지않음. Jaeho: I personally do not recall this specific event, but I do take these comments by heart. Thank you for the feedback\u0026mdash;I will take more care. 한 학기 동안 좋은 강의 감사했습니다. 결석 많이 해서 죄송합니다 ㅠ 정말 완벽한 과목과 완벽한 교수님이셨습니다. 딱 한가지 아쉬운 점은 과제가 더 많거나 시험이 있었으면 더 좋았을 것 같습니다. 굉장히 어려우나 얻을 점이 있습니다. 교수님 어려운내용을 쉽게 설명해주셔서 감사합니다. 교수님께서 강의를 매우 잘하십니다. "},{"id":6,"href":"/docs/teaching/comments/_effml_2022/","title":"Comments from Efficient ML (2022 Fall)","section":"Teaching","content":"Here are some comments from the students\u0026mdash;I take this comments by heart.\n각 토픽에 관한 관련 논문들을 매번 브리핑 해주시는 방식으로 수업이 진행되었는데, 세미나 같은 느낌으로 매우 잘 들었습니다. 좋은 강의 감사드립니다. efficient machine learning에 대한 폭넓은 이해를 할 수 있게 되었습니다. 해당 수업을 통해 연구에 큰 도움이 될 수 있었습니다. 또한 reproducibility challenge 과제도 신선하여 재미있게 할 수 있었습니다. 교수님이 매우 충실한 내용들로 채운 자료를 제공해 주신 점이 가장 좋았음. 교수님께서 열정적으로 피드백을 주시고, 강의를 해주십니다. 교수님 사랑합니다. 기대했던 것 이상의 수업이었습니다. 수업의 질도 높고 도움이 많이 되었습니다. 아침수업이 유일한 흠인 수업입니다. 한학기 수고하셨습니다! 수업이 syllabus에 적힌대로 체계적으로 잘 진행되었습니다. 감사합니다! 인공지능 연구에 대한 다양한 분야를 경험해볼 수있었습니다. 다양한 주제에 대한 동향을 접할 수 있는 좋은 수업이었습니다. 강의를 열정적으로 하십니다 교수님이 되게 열정적이시다. 수업 내용이 되게 트렌디함 이번 학기 동안 감사했습니다! 한 학기 동안 수업 감사했습니다! 최신 논문 기반의 수업과 페이스북 인턴 하셨던 선배님 강의는 이론뿐만 아니라 현업이야기도 들을 수 있어서 정말 의미있는 수업이였습니다. "}]