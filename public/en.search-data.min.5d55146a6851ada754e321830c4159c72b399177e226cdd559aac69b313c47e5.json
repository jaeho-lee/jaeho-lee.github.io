[{"id":0,"href":"/docs/research/","title":"ğŸ§ª Research","section":"Docs","content":"My long-term goal is to make AI more responsible\u0026mdash;accessible, sustainable, and righteous.\nCurrently, I am focusing on various facets of Efficient ML. Together with EffLers, I am striving to enable ML without giant-scale models/data/compute by innovating relevant theories, algorithms, and systems. Some recent research questions that I am specifically interested in are:\nHow (and to what extent) can we reduce the data-level redundancy for efficient training? How can we replace the noise inherent in ML with a compute-efficient one? Publications. See the group webpage.\n"},{"id":1,"href":"/docs/work/","title":"ğŸ§‘â€ğŸ’» Employment","section":"Docs","content":" Current # Assistant Professor EE@POSTECH (adj. AI, DST)\n2022.03\u0026ndash;Present Adjunct Professor, iCreate@Yonsei\n2022.09\u0026ndash;Present Visiting Faculty Researcher, Google\n2023.09\u0026ndash;Present Previous # Postdoctoral Researcher, EE@KAIST (ì „ë¬¸ì—°êµ¬ìš”ì›)\nHost: Jinwoo Shin\n2019.03\u0026ndash;2022.03 ASAN Fellow, Center for Data Analysis @ The Heritage Foundation\nHost: Salim Furth, James Sherk\n2013.05\u0026ndash;2013.07 "},{"id":2,"href":"/docs/education/","title":"ğŸ“ Education","section":"Docs","content":" Ph.D. in Electrical and Computer Engineering (2019) # Alma Mater. University of Illinois at Urbana-Champaign\nAdvisor. Maxim Raginsky\nThesis. \u0026ldquo;Robustness and Generalization Guarantees for Statistical Learning of Generative Models\u0026rdquo;\nM.S. in Electrical and Computer Engineering (2015) # Alma Mater. University of Illinois at Urbana-Champaign\nAdvisor. Maxim Raginsky\nThesis. \u0026ldquo;MMSE estimation from Quantized Observations in the Nonasymptotic Regime\u0026rdquo;\nB.S. in Electrical Engineering, Management Science (2013) # Alma Mater. Korea Advanced Institute of Science and Technology\nAdvisor. Yung Yi\ndouble major, summa cum laude\n"},{"id":3,"href":"/docs/teaching/","title":"ğŸ‘¨â€ğŸ« Teaching","section":"Docs","content":" 2023 Fall # EECE454 Introduction to Machine Learning Systems\n2023 Spring # EECE695D Deep Learning Theory (comments from students)\n2022 Fall # EECE695D-01 Efficient Machine Learning Systems (comments from students)\n(EduTech Teaching Excellence Award)\nPast Courses # As a TA, I have taught:\nFA17 TA, ECE563@UIUC Information theory. SP17 TA, ECE498@UIUC Introduction to stochastic systems. FA15 TA, ECE598@UIUC Statistical learning theory. "},{"id":4,"href":"/docs/teaching/fall23/","title":"23F - Intro to ML Sys","section":"ğŸ‘¨â€ğŸ« Teaching","content":" EECE454: Introduction to ML Systems (Fall 2023) # Team # Instructor. Jaeho Lee ì´ì¬í˜¸\nfirstname.lastname@postech.ac.kr TA. Minkyu Kim ê¹€ë¯¼ê·œ\nfirstname.lastname@postech.ac.kr Location \u0026amp; Time # Class. Mondays/Wednesdays, 09:30AM\u0026ndash;10:45AM @ 106 LG Hall Office Hr. Mondays, 04:00PM\u0026ndash;05:00PM @ 407 Eng Bldg 2 Schedule (tentative) # W1. Introduction / Basic Linear Algebra\n(9/4, 9/6) W2. Basic Probability / Simple Models\n(9/11, 9/13) W3. Simple Models / Support Vector Machines\n(9/18, 9/20) W4. Kernel SVM / K-Means Clustering\n(9/25, 9/27) W5. Gaussian Mixture Models\n(10/2, 10/4) W6. Decision Tree\n(10/9, 10/11) W7. Dimensionality Reduction\n(10/16, 10/18) W8. Mid-Term\n(10/23, 10/25) W9. Deep Learning Basics / Convolutional Networks\n(10/30, 11/1) W10. Backprop / Neural Network Training\n(11/6, 11/8) W11. Neural Network Training / -\n(11/13, 11/15) W12. Deep ConvNets / Generative Models\n(11/20, 11/22) W13. Generative Models / Language Models\n(11/27, 11/29) W14. Language Models / Multimodal Learning\n(12/4, 12/6) W15. Efficient ML / Deep Learning Theory\n(12/11, 12/13) W16. Final Project Presentation\n(12/18, 12/20) "},{"id":5,"href":"/docs/teaching/spring24/","title":"24S - (under construction)","section":"ğŸ‘¨â€ğŸ« Teaching","content":" EECE695E: Efficient ML Systems (Spring 2024) # Team # Instructor. Jaeho Lee ì´ì¬í˜¸\nfirstname.lastname@postech.ac.kr TA. (forthcoming) Location \u0026amp; Time # Class. (forthcoming) Office Hr. (forthcoming) What we\u0026rsquo;ll cover (hopefully) # Deep Learning Basics Architectures and Counting FLOPs Hardware bits Making models smaller Quantization Pruning \u0026amp; Sparsity Neural Architecture Search How to utilize the experience of other models Transfer Learning \u0026amp; Distillation Meta-Learning \u0026amp; Test-time Training Model Merging \u0026amp; Stitching Hyperparameter Transfer Prompt Tuning Organizing Large-scale Learning Parallelism \u0026amp; Pipelining Federated Learning Data Efficiency Data Compression Dataset Distillation \u0026amp; Condensation / SeiT Tips \u0026amp; Tricks for Large Transformers KV cache / FlashAttention / PagedAttention Speculative Decoding / Medusa Continuous Batching Schedule (tentative) # (forthcoming)\n"},{"id":6,"href":"/docs/misc/","title":"ï¹– Misc.","section":"Docs","content":" Talks (selected) # Tutorial, KICS Fall Conference, Gyeongju (2023.11.22)\n\u0026ldquo;Compressing Giant Neural Networks\u0026rdquo; Talk, Optimization \u0026amp; Machine Learning Workshop, UNIST (2023.08.24)\n\u0026ldquo;Problems that Arise in the Optimization of Neural Fields\u0026rdquo; Tutorial, KAIA Summer Conference, Yeosu (2023.07.17)\n\u0026ldquo;Recent Trends in Neural Data Compression: From Generative Models to Neural Fields\u0026rdquo; Tutorial, Korea SatCom Forum, Seoul (2023.05.31)\n\u0026ldquo;Recent Trends in Large Language Models\u0026rdquo; Miscellaneous Writings # Greece: Austerity Doesn\u0026rsquo;t Involve Public-Sector Layoffs\nJaeho Lee\nThe Daily Signal, 2023 (The Orange County Register, 2023)\në‚˜ë§Œì˜ ëª©í‘œ, ë‚˜ë§Œì˜ ê¸°ì¤€\nì´ì¬í˜¸\në™ì•„ì¼ë³´: ë‚´ê°€ ë§Œë‚œ ëª…ë¬¸ì¥, 2022\n"},{"id":7,"href":"/docs/teaching/comments/_dlt_2023/","title":"Comments from DLT (2023 Spring)","section":"ğŸ‘¨â€ğŸ« Teaching","content":"Here are some comments from the students\u0026mdash;I take this comments by heart.\nêµìˆ˜ë‹˜ê»˜ì„œ ìˆ˜ì—… ì¤€ë¹„ë¥¼ ì¶©ì‹¤íˆ í•˜ì‹  ê²ƒì´ ëŠê»´ì§‘ë‹ˆë‹¤. í•œ í•™ê¸° ë™ì•ˆ ìœ ìµí•œ ë‚´ìš©ì„ ë°°ìš¸ ìˆ˜ ìˆëŠ” ì¢‹ì€ ê¸°íšŒì˜€ìŠµë‹ˆë‹¤. ì¢‹ì€ ê°•ì˜ ê°ì‚¬í•©ë‹ˆë‹¤~ ìˆ˜ì—… ë‚´ìš©ì´ ì–´ë ¤ì›Œ ê°€ë” ë”°ë¼ê°€ê¸° ì–´ë ¤ì› ìŒ. ê³¼ì œ ë‚´ìš©ì€ ìˆ˜ì—…ê³¼ ê´€ë ¨ì´ ìˆì§€ë§Œ, ê°œì¸ì ìœ¼ë¡œëŠ” ê°•ì˜ ìŠ¬ë¼ì´ë“œì™€ ìˆ˜ì—… êµì¬ë§Œ ë³´ê³  ê³¼ì œë¥¼ í’€ ìˆ˜ ì—†ì—ˆë˜ ê²½ìš°ê°€ ë§ì•˜ê¸° ë•Œë¬¸ì—, ë‹¤ë¥¸ êµì¬ë“¤ì„ ì°¸ê³ í•˜ê³  ë”°ë¡œ ê³µë¶€í•˜ëŠ” ì‹œê°„ì„ ë§ì´ ê°€ì§ˆ ìˆ˜ ë°–ì— ì—†ì—ˆìŒ. ì´í•´ë§Œ ì˜ í•˜ê³  ì‹œê°„ íˆ¬ìë¥¼ ì¢€ ë” í•œë‹¤ë©´ ì´ë¡ ì ìœ¼ë¡œ ë§ì€ ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŒ. pptê°€ ì¢€ ë” highlevelì—ì„œì˜ íë¦„ì„ ì˜ ì´í•´í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. ì‹ ì „ê°œë¥¼ ì •ì‹  ì—†ì´ ë”°ë¼ì ë‹¤ê°€ ì •ì‘ ì–´ë–¤ ì‹ì„ ì „ê°œí–ˆëŠ”ì§€ ê·¸ ì˜ë¯¸ë¥¼ ì˜ ì´í•´í•˜ì§€ ëª»í•˜ê³  ì§€ë‚˜ê°€ë²„ë¦¬ëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ ì•„ì‰¬ì› ìŠµë‹ˆë‹¤. Jaeho: Thank you for the great suggestion. I will try to add some \u0026ldquo;overview\u0026rdquo; slides in the beginning and the end of all lectures. ì¢‹ì€ ê°•ì˜ë¥¼ í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì—°êµ¬ì— analyzingì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ì´ˆë¥¼ ë§Œë“¤ì–´ ì£¼ì…¨ìŠµë‹ˆë‹¤! êµìˆ˜ë‹˜ê»˜ì„œ ìˆ˜ì—… ì¤€ë¹„ì— ì‹ ê²½ì“°ì‹ ê²ƒì´ ëŠê»´ì§€ëŠ” ìˆ˜ì—…ì…ë‹ˆë‹¤. êµìˆ˜ë‹˜ê»˜ì„œ ì–´ë ¤ìš´ ë¶„ì•¼ì— ëŒ€í•´ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ê³¼ëª©ì„ êµ¬ì„±í•˜ì‹œê³  ì„¤ëª…ì„ ìì„¸íˆ í•´ì£¼ì…¨ê¸° ë•Œë¬¸ì— ì¢‹ì€ ê³¼ëª©ì´ì—ˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë‹¤ë§Œ, ê°œì¸ì ìœ¼ë¡œëŠ” ë¶„ì•¼ê°€ ì–´ë µê³  ì œê°€ ìµìˆ™í•˜ì§€ ì•Šì€ ì  ë•Œë¬¸ì— ì´í•´ê°€ ì–´ë ¤ìš´ ë¶€ë¶„ì´ ë§ì•˜ë˜ ë¶€ë¶„ì´ ì•„ì‰¬ìš´ ì ì´ì—ˆë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë”¥ëŸ¬ë‹ ë’¤ì— ìˆëŠ” ì´ë¡ ì— ëŒ€í•´ ì•Œê²Œ ë˜ì–´ ì •ë§ ìœ ìµí–ˆìŠµë‹ˆë‹¤. ì •ë§ í¥ë¯¸ë¡œì› ê³  ì´ëŸ° ìˆ˜ì—…ì´ ë§ì´ ìƒê²¼ìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤ ì•„ì£¼ í¥ë¯¸ë¡­ì§€ë§Œ ì–´ë ¤ìš´ ë‚´ìš©ì„ ìµœëŒ€í•œ ì˜ í’€ì–´ë‚´ì–´ ì„¤ëª…í•˜ì‹­ë‹ˆë‹¤. ìˆ˜ì—… ë¡œë“œ ë˜í•œ í•™ìƒì„ ì˜ ë°°ë ¤í•´ ì£¼ì‹­ë‹ˆë‹¤. ìµœê³ ì˜ êµìˆ˜ë‹˜ì´ì‹­ë‹ˆë‹¤. The professor has a very good command in English. The lectures are challenging, but well designed. I do recommend this course for people who are interested in ML/DL or just the theoretical aspect of things. Being able to implement ML/DL algorithm is nice, but having a strong foundation in theoretical background do give you an edge, not only in research but also when working in the industry. í˜•ì‹ì ì¸ ê³¼ëª©ë“¤ì„ ë“¤ì„ë•ŒëŠ” ì‹œê°„ë„ ì•„ê¹ê³  í˜„íƒ€ê°€ ì™”ëŠ”ë°, ì´ ìˆ˜ì—…ì€ ë§¤ìš° ìœ ìµí–ˆê³  ì´ëŸ¬ê¸° ìœ„í•´ ëŒ€í•™ì›ì— ì™”ì§€ë¼ëŠ” ë§Œì¡±ê°ë„ ë“¤ì—ˆìŠµë‹ˆë‹¤. ëª©ì†Œë¦¬ëŠ” ì˜ ì•ˆë“¤ë¦¬ê³  ìˆ˜ì—…ì‹œê°„ì— ì´ì•¼ê¸° í•´ì¤€ ë‚´ìš©ì„ ì´í•´ê°€ ì˜ ì•ˆë˜ì„œ ë©”ì¼ë¡œ ì§ˆë¬¸í•˜ë©´ ìˆ˜ì—…ì—ì„œ ë‹¤ í•œ ì–˜ê¸°ë¼ê³ , ìˆ˜ì—…ì„ ì•ˆë“¤ì—ˆëƒê³  ë§í•˜ë©´ì„œ ê²°êµ­ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì£¼ì§€ì•ŠìŒ. Jaeho: I personally do not recall this specific event, but I do take these comments by heart. Thank you for the feedback\u0026mdash;I will take more care. í•œ í•™ê¸° ë™ì•ˆ ì¢‹ì€ ê°•ì˜ ê°ì‚¬í–ˆìŠµë‹ˆë‹¤. ê²°ì„ ë§ì´ í•´ì„œ ì£„ì†¡í•©ë‹ˆë‹¤ ã…  ì •ë§ ì™„ë²½í•œ ê³¼ëª©ê³¼ ì™„ë²½í•œ êµìˆ˜ë‹˜ì´ì…¨ìŠµë‹ˆë‹¤. ë”± í•œê°€ì§€ ì•„ì‰¬ìš´ ì ì€ ê³¼ì œê°€ ë” ë§ê±°ë‚˜ ì‹œí—˜ì´ ìˆì—ˆìœ¼ë©´ ë” ì¢‹ì•˜ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. êµ‰ì¥íˆ ì–´ë ¤ìš°ë‚˜ ì–»ì„ ì ì´ ìˆìŠµë‹ˆë‹¤. êµìˆ˜ë‹˜ ì–´ë ¤ìš´ë‚´ìš©ì„ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. êµìˆ˜ë‹˜ê»˜ì„œ ê°•ì˜ë¥¼ ë§¤ìš° ì˜í•˜ì‹­ë‹ˆë‹¤. "},{"id":8,"href":"/docs/teaching/comments/_effml_2022/","title":"Comments from Efficient ML (2022 Fall)","section":"ğŸ‘¨â€ğŸ« Teaching","content":"Here are some comments from the students\u0026mdash;I take this comments by heart.\nê° í† í”½ì— ê´€í•œ ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ë§¤ë²ˆ ë¸Œë¦¬í•‘ í•´ì£¼ì‹œëŠ” ë°©ì‹ìœ¼ë¡œ ìˆ˜ì—…ì´ ì§„í–‰ë˜ì—ˆëŠ”ë°, ì„¸ë¯¸ë‚˜ ê°™ì€ ëŠë‚Œìœ¼ë¡œ ë§¤ìš° ì˜ ë“¤ì—ˆìŠµë‹ˆë‹¤. ì¢‹ì€ ê°•ì˜ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. efficient machine learningì— ëŒ€í•œ í­ë„“ì€ ì´í•´ë¥¼ í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. í•´ë‹¹ ìˆ˜ì—…ì„ í†µí•´ ì—°êµ¬ì— í° ë„ì›€ì´ ë  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë˜í•œ reproducibility challenge ê³¼ì œë„ ì‹ ì„ í•˜ì—¬ ì¬ë¯¸ìˆê²Œ í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. êµìˆ˜ë‹˜ì´ ë§¤ìš° ì¶©ì‹¤í•œ ë‚´ìš©ë“¤ë¡œ ì±„ìš´ ìë£Œë¥¼ ì œê³µí•´ ì£¼ì‹  ì ì´ ê°€ì¥ ì¢‹ì•˜ìŒ. êµìˆ˜ë‹˜ê»˜ì„œ ì—´ì •ì ìœ¼ë¡œ í”¼ë“œë°±ì„ ì£¼ì‹œê³ , ê°•ì˜ë¥¼ í•´ì£¼ì‹­ë‹ˆë‹¤. êµìˆ˜ë‹˜ ì‚¬ë‘í•©ë‹ˆë‹¤. ê¸°ëŒ€í–ˆë˜ ê²ƒ ì´ìƒì˜ ìˆ˜ì—…ì´ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ì—…ì˜ ì§ˆë„ ë†’ê³  ë„ì›€ì´ ë§ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ì¹¨ìˆ˜ì—…ì´ ìœ ì¼í•œ í ì¸ ìˆ˜ì—…ì…ë‹ˆë‹¤. í•œí•™ê¸° ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤! ìˆ˜ì—…ì´ syllabusì— ì íŒëŒ€ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì˜ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤! ì¸ê³µì§€ëŠ¥ ì—°êµ¬ì— ëŒ€í•œ ë‹¤ì–‘í•œ ë¶„ì•¼ë¥¼ ê²½í—˜í•´ë³¼ ìˆ˜ìˆì—ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•œ ë™í–¥ì„ ì ‘í•  ìˆ˜ ìˆëŠ” ì¢‹ì€ ìˆ˜ì—…ì´ì—ˆìŠµë‹ˆë‹¤. ê°•ì˜ë¥¼ ì—´ì •ì ìœ¼ë¡œ í•˜ì‹­ë‹ˆë‹¤ êµìˆ˜ë‹˜ì´ ë˜ê²Œ ì—´ì •ì ì´ì‹œë‹¤. ìˆ˜ì—… ë‚´ìš©ì´ ë˜ê²Œ íŠ¸ë Œë””í•¨ ì´ë²ˆ í•™ê¸° ë™ì•ˆ ê°ì‚¬í–ˆìŠµë‹ˆë‹¤! í•œ í•™ê¸° ë™ì•ˆ ìˆ˜ì—… ê°ì‚¬í–ˆìŠµë‹ˆë‹¤! ìµœì‹  ë…¼ë¬¸ ê¸°ë°˜ì˜ ìˆ˜ì—…ê³¼ í˜ì´ìŠ¤ë¶ ì¸í„´ í•˜ì…¨ë˜ ì„ ë°°ë‹˜ ê°•ì˜ëŠ” ì´ë¡ ë¿ë§Œ ì•„ë‹ˆë¼ í˜„ì—…ì´ì•¼ê¸°ë„ ë“¤ì„ ìˆ˜ ìˆì–´ì„œ ì •ë§ ì˜ë¯¸ìˆëŠ” ìˆ˜ì—…ì´ì˜€ìŠµë‹ˆë‹¤. "}]