[{"id":0,"href":"/docs/research/","title":"🧪 Research","section":"Docs","content":"My long-term goal is to make AI more responsible\u0026mdash;accessible, sustainable, and righteous.\nCurrently, I am focusing on various facets of Efficient ML. Together with EffLers, I am striving to enable ML without giant-scale models/data/compute by innovating relevant theories, algorithms, and systems. Some recent research questions that I am specifically interested in are:\nHow (and to what extent) can we reduce the data-level redundancy for efficient training? How can we replace the noise inherent in ML with a compute-efficient one? Publications. See the group webpage.\n"},{"id":1,"href":"/docs/work/","title":"🧑‍💻 Employment","section":"Docs","content":" Current # Assistant Professor EE@POSTECH (adj. AI, DST)\n2022.03\u0026ndash;Present Adjunct Professor, iCreate@Yonsei\n2022.09\u0026ndash;Present Visiting Faculty Researcher, Google\n2023.09\u0026ndash;Present Previous # Postdoctoral Researcher, EE@KAIST (전문연구요원)\nHost: Jinwoo Shin\n2019.03\u0026ndash;2022.03 ASAN Fellow, Center for Data Analysis @ The Heritage Foundation\nHost: Salim Furth, James Sherk\n2013.05\u0026ndash;2013.07 "},{"id":2,"href":"/docs/education/","title":"🎓 Education","section":"Docs","content":" Ph.D. in Electrical and Computer Engineering (2019) # Alma Mater. University of Illinois at Urbana-Champaign\nAdvisor. Maxim Raginsky\nThesis. \u0026ldquo;Robustness and Generalization Guarantees for Statistical Learning of Generative Models\u0026rdquo;\nM.S. in Electrical and Computer Engineering (2015) # Alma Mater. University of Illinois at Urbana-Champaign\nAdvisor. Maxim Raginsky\nThesis. \u0026ldquo;MMSE estimation from Quantized Observations in the Nonasymptotic Regime\u0026rdquo;\nB.S. in Electrical Engineering, Management Science (2013) # Alma Mater. Korea Advanced Institute of Science and Technology\nAdvisor. Yung Yi\ndouble major, summa cum laude\n"},{"id":3,"href":"/docs/teaching/","title":"👨‍🏫 Teaching","section":"Docs","content":" 2023 Fall # EECE454 Introduction to Machine Learning Systems\n2023 Spring # EECE695D Deep Learning Theory (comments from students)\n2022 Fall # EECE695D-01 Efficient Machine Learning Systems (comments from students)\n(EduTech Teaching Excellence Award)\nPast Courses # As a TA, I have taught:\nFA17 TA, ECE563@UIUC Information theory. SP17 TA, ECE498@UIUC Introduction to stochastic systems. FA15 TA, ECE598@UIUC Statistical learning theory. "},{"id":4,"href":"/docs/teaching/fall23/","title":"23F - Intro to ML Sys","section":"👨‍🏫 Teaching","content":" EECE454: Introduction to ML Systems (Fall 2023) # Team # Instructor. Jaeho Lee 이재호\nfirstname.lastname@postech.ac.kr TA. Minkyu Kim 김민규\nfirstname.lastname@postech.ac.kr Location \u0026amp; Time # Class. Mondays/Wednesdays, 09:30AM\u0026ndash;10:45AM @ 106 LG Hall Office Hr. Mondays, 04:00PM\u0026ndash;05:00PM @ 407 Eng Bldg 2 Schedule (tentative) # W1. Introduction / Basic Linear Algebra\n(9/4, 9/6) W2. Basic Probability / Simple Models\n(9/11, 9/13) W3. Simple Models / Support Vector Machines\n(9/18, 9/20) W4. Kernel SVM / K-Means Clustering\n(9/25, 9/27) W5. Gaussian Mixture Models\n(10/2, 10/4) W6. Decision Tree\n(10/9, 10/11) W7. Dimensionality Reduction\n(10/16, 10/18) W8. Mid-Term\n(10/23, 10/25) W9. Deep Learning Basics / Convolutional Networks\n(10/30, 11/1) W10. Backprop / Neural Network Training\n(11/6, 11/8) W11. Neural Network Training / -\n(11/13, 11/15) W12. Deep ConvNets / Generative Models\n(11/20, 11/22) W13. Generative Models / Language Models\n(11/27, 11/29) W14. Language Models / Multimodal Learning\n(12/4, 12/6) W15. Efficient ML / Deep Learning Theory\n(12/11, 12/13) W16. Final Project Presentation\n(12/18, 12/20) "},{"id":5,"href":"/docs/teaching/spring24/","title":"24S - (under construction)","section":"👨‍🏫 Teaching","content":" EECE695E: Efficient ML Systems (Spring 2024) # Team # Instructor. Jaeho Lee 이재호\nfirstname.lastname@postech.ac.kr TA. (forthcoming) Location \u0026amp; Time # Class. (forthcoming) Office Hr. (forthcoming) What we\u0026rsquo;ll cover (hopefully) # Deep Learning Basics Architectures and Counting FLOPs Hardware bits Making models smaller Quantization Pruning \u0026amp; Sparsity Neural Architecture Search How to utilize the experience of other models Transfer Learning \u0026amp; Distillation Meta-Learning \u0026amp; Test-time Training Model Merging \u0026amp; Stitching Hyperparameter Transfer Prompt Tuning Organizing Large-scale Learning Parallelism \u0026amp; Pipelining Federated Learning Data Efficiency Data Compression Dataset Distillation \u0026amp; Condensation / SeiT Tips \u0026amp; Tricks for Large Transformers KV cache / FlashAttention / PagedAttention Speculative Decoding / Medusa Continuous Batching Schedule (tentative) # (forthcoming)\n"},{"id":6,"href":"/docs/misc/","title":"﹖ Misc.","section":"Docs","content":" Talks (selected) # Tutorial, KICS Fall Conference, Gyeongju (2023.11.22)\n\u0026ldquo;Compressing Giant Neural Networks\u0026rdquo; Talk, Optimization \u0026amp; Machine Learning Workshop, UNIST (2023.08.24)\n\u0026ldquo;Problems that Arise in the Optimization of Neural Fields\u0026rdquo; Tutorial, KAIA Summer Conference, Yeosu (2023.07.17)\n\u0026ldquo;Recent Trends in Neural Data Compression: From Generative Models to Neural Fields\u0026rdquo; Tutorial, Korea SatCom Forum, Seoul (2023.05.31)\n\u0026ldquo;Recent Trends in Large Language Models\u0026rdquo; Miscellaneous Writings # Greece: Austerity Doesn\u0026rsquo;t Involve Public-Sector Layoffs\nJaeho Lee\nThe Daily Signal, 2023 (The Orange County Register, 2023)\n나만의 목표, 나만의 기준\n이재호\n동아일보: 내가 만난 명문장, 2022\n"},{"id":7,"href":"/docs/teaching/comments/_dlt_2023/","title":"Comments from DLT (2023 Spring)","section":"👨‍🏫 Teaching","content":"Here are some comments from the students\u0026mdash;I take this comments by heart.\n교수님께서 수업 준비를 충실히 하신 것이 느껴집니다. 한 학기 동안 유익한 내용을 배울 수 있는 좋은 기회였습니다. 좋은 강의 감사합니다~ 수업 내용이 어려워 가끔 따라가기 어려웠음. 과제 내용은 수업과 관련이 있지만, 개인적으로는 강의 슬라이드와 수업 교재만 보고 과제를 풀 수 없었던 경우가 많았기 때문에, 다른 교재들을 참고하고 따로 공부하는 시간을 많이 가질 수 밖에 없었음. 이해만 잘 하고 시간 투자를 좀 더 한다면 이론적으로 많은 도움을 받을 수 있을 것 같음. ppt가 좀 더 highlevel에서의 흐름을 잘 이해할 수 있도록 구성되면 좋겠습니다. 식 전개를 정신 없이 따라적다가 정작 어떤 식을 전개했는지 그 의미를 잘 이해하지 못하고 지나가버리는 경우가 많아서 아쉬웠습니다. Jaeho: Thank you for the great suggestion. I will try to add some \u0026ldquo;overview\u0026rdquo; slides in the beginning and the end of all lectures. 좋은 강의를 해주셔서 감사합니다. 연구에 analyzing을 사용할 수 있는 기초를 만들어 주셨습니다! 교수님께서 수업 준비에 신경쓰신것이 느껴지는 수업입니다. 교수님께서 어려운 분야에 대해 이해하기 쉽도록 과목을 구성하시고 설명을 자세히 해주셨기 때문에 좋은 과목이었다고 생각합니다. 다만, 개인적으로는 분야가 어렵고 제가 익숙하지 않은 점 때문에 이해가 어려운 부분이 많았던 부분이 아쉬운 점이었던 것 같습니다. 딥러닝 뒤에 있는 이론에 대해 알게 되어 정말 유익했습니다. 정말 흥미로웠고 이런 수업이 많이 생겼으면 좋겠습니다 아주 흥미롭지만 어려운 내용을 최대한 잘 풀어내어 설명하십니다. 수업 로드 또한 학생을 잘 배려해 주십니다. 최고의 교수님이십니다. The professor has a very good command in English. The lectures are challenging, but well designed. I do recommend this course for people who are interested in ML/DL or just the theoretical aspect of things. Being able to implement ML/DL algorithm is nice, but having a strong foundation in theoretical background do give you an edge, not only in research but also when working in the industry. 형식적인 과목들을 들을때는 시간도 아깝고 현타가 왔는데, 이 수업은 매우 유익했고 이러기 위해 대학원에 왔지라는 만족감도 들었습니다. 목소리는 잘 안들리고 수업시간에 이야기 해준 내용을 이해가 잘 안되서 메일로 질문하면 수업에서 다 한 얘기라고, 수업을 안들었냐고 말하면서 결국 질문에 대한 답변을 주지않음. Jaeho: I personally do not recall this specific event, but I do take these comments by heart. Thank you for the feedback\u0026mdash;I will take more care. 한 학기 동안 좋은 강의 감사했습니다. 결석 많이 해서 죄송합니다 ㅠ 정말 완벽한 과목과 완벽한 교수님이셨습니다. 딱 한가지 아쉬운 점은 과제가 더 많거나 시험이 있었으면 더 좋았을 것 같습니다. 굉장히 어려우나 얻을 점이 있습니다. 교수님 어려운내용을 쉽게 설명해주셔서 감사합니다. 교수님께서 강의를 매우 잘하십니다. "},{"id":8,"href":"/docs/teaching/comments/_effml_2022/","title":"Comments from Efficient ML (2022 Fall)","section":"👨‍🏫 Teaching","content":"Here are some comments from the students\u0026mdash;I take this comments by heart.\n각 토픽에 관한 관련 논문들을 매번 브리핑 해주시는 방식으로 수업이 진행되었는데, 세미나 같은 느낌으로 매우 잘 들었습니다. 좋은 강의 감사드립니다. efficient machine learning에 대한 폭넓은 이해를 할 수 있게 되었습니다. 해당 수업을 통해 연구에 큰 도움이 될 수 있었습니다. 또한 reproducibility challenge 과제도 신선하여 재미있게 할 수 있었습니다. 교수님이 매우 충실한 내용들로 채운 자료를 제공해 주신 점이 가장 좋았음. 교수님께서 열정적으로 피드백을 주시고, 강의를 해주십니다. 교수님 사랑합니다. 기대했던 것 이상의 수업이었습니다. 수업의 질도 높고 도움이 많이 되었습니다. 아침수업이 유일한 흠인 수업입니다. 한학기 수고하셨습니다! 수업이 syllabus에 적힌대로 체계적으로 잘 진행되었습니다. 감사합니다! 인공지능 연구에 대한 다양한 분야를 경험해볼 수있었습니다. 다양한 주제에 대한 동향을 접할 수 있는 좋은 수업이었습니다. 강의를 열정적으로 하십니다 교수님이 되게 열정적이시다. 수업 내용이 되게 트렌디함 이번 학기 동안 감사했습니다! 한 학기 동안 수업 감사했습니다! 최신 논문 기반의 수업과 페이스북 인턴 하셨던 선배님 강의는 이론뿐만 아니라 현업이야기도 들을 수 있어서 정말 의미있는 수업이였습니다. "}]