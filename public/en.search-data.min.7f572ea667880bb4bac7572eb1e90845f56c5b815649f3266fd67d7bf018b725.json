[{"id":0,"href":"/docs/advising/current/","title":"Current","section":"Advising","content":" Graduate Students # @ Pohang üèùÔ∏è # Jiwoon Lee (2022.02\u0026ndash;) Efficient ML in the Wild\nJunwon Seo (2022.02\u0026ndash;) Accelerating Neural Field Generation\nHagyeong Lee (2022.09\u0026ndash;) Generative Data Compression\nMinkyu Kim (2023.02\u0026ndash;) Multimodal Data Compression\nYu Ji Byun (2023.02\u0026ndash;) Learning with High-Resolution Video\nJuyun Wee (2023.09\u0026ndash;) Learning from Extremely Long Sequences\nEffLer on Leave @ Google üè¢ # Seungwoo Son (2022.02\u0026ndash;) Large-Scale Model Compression\nInterns (2023 Fall) # Sangyoon Lee Fast Neural Field Generation\nJiyun Bae (remote) Visual Prompt Tuning\nSangbeom Ha Large-Scale Transformer Quantization\nMinyoung Kang Neural Cellular Automata\nYousung Roh TBD\n"},{"id":1,"href":"/docs/teaching/fall23/","title":"EECE454-23F","section":"Teaching","content":" EECE454: Introduction to ML Systems (Fall 2023) # Team # Instructor. Jaeho Lee Ïù¥Ïû¨Ìò∏\nfirstname.lastname@postech.ac.kr TA. Minkyu Kim ÍπÄÎØºÍ∑ú\nfirstname.lastname@postech.ac.kr Location \u0026amp; Time # Class. Mondays/Wednesdays, 09:30AM\u0026ndash;10:45AM @ 106 LG Hall Office Hr. Wednesdays, 10:50AM\u0026ndash;11:50AM @ 407 Eng Bldg 2 Schedule (tentative) # W1. Introduction / Basic Linear Algebra\n(9/4, 9/6) W2. Basic Probability / Simple Models\n(9/11, 9/13) W3. Simple Models / Support Vector Machines\n(9/18, 9/20) W4. K-Means Clustering\n(9/25, 9/27) W5. Mixture Models\n(10/2, 10/4) W6. Mixture Models (cont\u0026rsquo;d)\n(10/9, 10/11) W7. Dimensionality Reduction\n(10/16, 10/18) W8. Mid-Term\n(10/23, 10/25) W9. Perceptrons / Neural Networks\n(10/30, 11/1) W10. GD and Backprop\n(11/6, 11/8) W11. Convolutional Models\n(11/13, 11/15) W12. Generative Models\n(11/20, 11/22) W13. Vision Applications\n(11/27, 11/29) W14. Language Models\n(12/4, 12/6) W15. Multimodal Learning\n(12/11, 12/13) W16. Final Project Presentation\n(12/18, 12/20) "},{"id":2,"href":"/docs/research/","title":"Research","section":"Docs","content":"My long-term goal is to make AI more responsible\u0026mdash;accessible, sustainable, and righteous.\nCurrently, I am focusing on various facets of Efficient ML. Together with EffLers, I am striving to enable ML without giant-scale models/data/compute by innovating relevant theories, algorithms, and systems. Some recent research questions that I am specifically interested in are:\nHow (and to what extent) can we reduce the data-level redundancy for efficient training? How can we replace the noise inherent in ML with a compute-efficient one? 2023 # Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling\nJunhyun Nam, Sangwoo Mo, Jaeho Lee, and Jinwoo Shin\nTMLR 2023 (ICML 2023 Workshop: Spurious Correlations, Invariance, and Stability)\nModality-Agnostic Variational Compression of Implicit Neural Representations\nJonathan R. Schwarz, Jihoon Tack, Yee Whye Teh, Jaeho Lee, and Jinwoo Shin\nICML 2023 (ICLR 2023 Workshop: Neural Fields across Fields)\nBias-to-Text: Debiasing Unknown Visual Biases through Language Interpretation\nYounghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, and Jinwoo Shin\nICML 2023 Workshop: Spurious Correlations, Invariance, and Stability\nOn the Effectiveness of Sharpness-aware Minimization with Large Mini-batches\nJinseok Chung, Seonghwan Park, Jaeho Lee, and Namhoon Lee\nICML 2023 Workshop: High-Dimensional Learning Dynamics\nMaskedKD: Efficient Distillation of Vision Transformers with Masked Images\nSeungwoo Son, Namhoon Lee, and Jaeho Lee\nICLR 2023 Workshop: Sparsity in Neural Networks (IPIU 2023 Oral ü•â)\nLearning Large-scale Neural Fields via Context Pruned Meta-learning\nJihoon Tack, Subin Kim, Sihyun Yu, Jaeho Lee, Jinwoo Shin, and Jonathan R. Schwarz\nICLR 2023 Workshop: Neural Fields across Fields\nCommunication-Efficient Split Learning via Adaptive Feature-wise Compression\nYongjeong Oh, Jaeho Lee, Christopher G. Brinton, and Yo-Seb Jeon\nUnder Review\nDebiased Distillation by Transplanting the Last Layer\nJiwoon Lee and Jaeho Lee\narXiv preprint 2302.11187 (IPIU 2023)\n2022 # Scalable Neural Video Representations with Learnable Positional Features\nSubin Kim, Sihyun Yu, Jaeho Lee, and Jinwoo Shin\nNeurIPS 2022 (project page)\nMeta-learning with Self-improving Momentum Targets\nJihoon Tack, Jongjin Park, Hankook Lee, Jaeho Lee and Jinwoo Shin\nNeurIPS 2022\nSpread Spurious Attribute: Improving Worst-Group Accuracy with Spurious Attribute Estimation\nJunhyun Nam, Jaehyung Kim, Jaeho Lee, and Jinwoo Shin\nICLR 2022\nZero-shot Blind Image Denoising via Implicit Neural Representations\nChaewon Kim, Jaeho Lee, and Jinwoo Shin\narXiv preprint 2204.02405\n2021 # Meta-learning Sparse Implicit Neural Representations\nJaeho Lee, Jihoon Tack, Namhoon Lee, and Jinwoo Shin\nNeurIPS 2021\nCo2L: Contrastive Continual Learning\nHyuntak Cha, Jaeho Lee, and Jinwoo Shin\nICCV 2021\nProvable Memorization via Deep Neural Networks using Sub-linear Parameters\nSejun Park, Jaeho Lee, Chulhee Yun, and Jinwoo Shin\nCOLT 2021 (DeepMath 2020 Oral)\nMinimum Width for Universal Approximation\nSejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin\nICLR 2021 Spotlight (DeepMath 2020 Oral)\nLayer-adaptive Sparsity for the Magnitude-based Pruning\nJaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin\nICLR 2021\nMASKER: Masked Keyword Regularization for Reliable Text Generation\nSeung Jun Moon, Sangwoo Mo, Kimin Lee, Jaeho Lee, and Jinwoo Shin\nAAAI 2021\nGreedyprune: Layer-wise Optimization Algorithms for Magnitude-based Pruning\nVinoth Nandakumar and Jaeho Lee\nSparse Neural Network Workshop 2021\n2020 # Learning Bounds for Risk-sensitive Learning\nJaeho Lee, Sejun Park, and Jinwoo Shin\nNeurIPS 2020\nLearning from Failure: Training Debiased Classifier from Biased Classifier\nJunhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin\nNeurIPS 2020\nLookahead: A Far-sighted Alternative of Magnitude-based Pruning\nSejun Park, Jaeho Lee, Sangwoo Mo, and Jinwoo Shin\nICLR 2020\nPre-2020 # Learning Finite-dimensional Coding Schemes with Nonlinear Reconstruction Maps\nJaeho Lee and Maxim Raginsky\nSIMODS 2019\nMinimax Statistical Learning with Wasserstein Distances\nJaeho Lee and Maxim Raginsky\nNeurIPS 2018 Spotlight\nOn MMSE Estimation from Quantized Observations in the Nonasymptotic Regime\nJaeho Lee, Maxim Raginsky, and Pierre Moulin\nISIT 2015\nDomestic Posters # An Empirical Study on the Bias of Generative Image Compression\nHagyeong Lee and Jaeho Lee\nIPIU 2023\nIs Sparse Identification Model Sufficiently Biased?\nJunwon Seo and Jaeho Lee\nIPIU 2023\n"},{"id":3,"href":"/docs/advising/","title":"Advising","section":"Docs","content":"I am advising brilliant talents at EffL@POSTECH.\nAs an advisor, I am commited to these principles:\nkeeping the group small, having individual weekly meetings (30-60mins at least), and keeping non-academic burdens minimal. I expect EffLers to\nlead their own research agenda patiently, gain top-notch expertise in their own area, and willingly share their knowledge with fellow EffLers. "},{"id":4,"href":"/docs/advising/past/","title":"Past","section":"Advising","content":" Interns # 2023 Summer # Jegwang Ryu\nJiyun Bae\nSangyoon Lee\nDohyun Kim\nSangbeom Ha\n2023 Spring # Soochang Song\nJuyun Wee (‚Üí EffL)\n2022 Winter # Jeonghun Cho\nSoochang Song\n2021 Winter # Seyeon Park (‚Üí Yonsei)\nHagyeong Lee (‚Üí EffL)\n"},{"id":5,"href":"/docs/advising/future/","title":"Apply?","section":"Advising","content":"We have a limited number of openings for graduate students, interns, and postdocs.\n(last updated: 08.31.2023)\nHow to Apply? # Before anything, please email me with your\nTranscript (with rank) CV/R√©sum√© A short statement of your research interests We will have a short coffee chat to discuss the next steps.\n"},{"id":6,"href":"/docs/teaching/","title":"Teaching","section":"Docs","content":" 2023 Fall # EECE454 Introduction to Machine Learning Systems\n2023 Spring # EECE695D Deep Learning Theory\n2022 Fall # EECE695D-01 Efficient Machine Learning Systems\n(EduTech Teaching Excellence Award)\nPast Courses # As a TA, I have taught:\nFA17 TA, ECE563@UIUC Information theory. SP17 TA, ECE498@UIUC Introduction to stochastic systems. FA15 TA, ECE598@UIUC Statistical learning theory. "},{"id":7,"href":"/docs/misc/","title":"Misc.","section":"Docs","content":" Employment # Assistant Professor, EE@POSTECH (adj. AI, DST)\n2022.03\u0026ndash;Present Adjunct Professor, iCreate@Yonsei\n2022.09\u0026ndash;Present Visiting Faculty Researcher, Google\n2023.09\u0026ndash;Present Previous Appointments # Postdoctoral Researcher, EE@KAIST (Ï†ÑÎ¨∏Ïó∞Íµ¨ÏöîÏõê)\nHost: Jinwoo Shin\n2019.03\u0026ndash;2022.03 ASAN Fellow, Center for Data Analysis @ The Heritage Foundation\nHost: Salim Furth, James Sherk\n2013.05\u0026ndash;2013.07 Education # Ph.D., Electrical and Computer Engineering (2019)\nUniversity of Illinois at Urbana-Champaign\nThesis: \u0026ldquo;Robustness and Generalization Guarantees for Statistical Learning of Generative Models\u0026rdquo;\nAdvisor: Maxim Raginsky M.S., Electrical and Computer Engineering (2015)\nUniversity of Illinois at Urbana-Champaign\nThesis: \u0026ldquo;MMSE estimation from Quantized Observations in the Nonasymptotic Regime\u0026rdquo;\nAdvisor: Maxim Raginsky B.S. Electrical Engineering, Management Science (2013)\nKorea Advanced Institute of Science and Technology\ndouble major, summa cum laude\nAdvisor: Yung Yi Talks (selected) # Tutorial, KICS Fall Conference, Gyeongju (2023.11.22)\n\u0026ldquo;Compressing Giant Neural Networks\u0026rdquo; Talk, Optimization \u0026amp; Machine Learning Workshop, UNIST (2023.08.24)\n\u0026ldquo;Problems that Arise in the Optimization of Neural Fields\u0026rdquo; Tutorial, KAIA Summer Conference, Yeosu (2023.07.17)\n\u0026ldquo;Recent Trends in Neural Data Compression: From Generative Models to Neural Fields\u0026rdquo; Tutorial, Korea SatCom Forum, Seoul (2023.05.31)\n\u0026ldquo;Recent Trends in Large Language Models\u0026rdquo; Miscellaneous Writings # Greece: Austerity Doesn\u0026rsquo;t Involve Public-Sector Layoffs\nJaeho Lee\nThe Daily Signal, 2023 (The Orange County Register, 2023)\nÎÇòÎßåÏùò Î™©Ìëú, ÎÇòÎßåÏùò Í∏∞Ï§Ä\nÏù¥Ïû¨Ìò∏\nÎèôÏïÑÏùºÎ≥¥: ÎÇ¥Í∞Ä ÎßåÎÇú Î™ÖÎ¨∏Ïû•, 2022\n"}]