[{"id":0,"href":"/docs/research/","title":"Research","section":"Docs","content":"Long-term Goal. Making AI more responsible: accessible, sustainable, and righteous.\nCurrent Focus. Various facets of effcient machine learning.\nIn particular, I try to answer these questions.\nGiven a fixed resource for training and inference, what is the best machine intelligence we can get? How can we effectively utilize the data-level redundancy for efficient training? How can we replace the noise, inherent in ML, with a compute-efficient option? Papers. See the group webpage.\n"},{"id":1,"href":"/docs/teaching/fall24dlt/","title":"24F - DL Theory","section":"Teaching","content":" EECE695D: Deep Learning Theory (Spring 2024) # This semester, this course is:\n(1) jointly served at POSTECH and Yonsei\n(2) an online course\n(3) taught in Korean\nTeam # Instructor @ P. Jaeho Lee ì´ì¬í˜¸ âœ‰ï¸ Instructor @ Y. Jy-yong Sohn ì†ì§€ìš© âœ‰ï¸ TA @ P. Kyumin Kim ê¹€ê·œë¯¼ âœ‰ï¸ TA @ Y. Chungpa Lee ì´ì²­íŒŒ âœ‰ï¸ Contributor. Taesun Yeom ì—¼íƒœì„  âœ‰ï¸ Location \u0026amp; Time # Class. Online at PLMSğŸ”— or LearnUsğŸ”—. Uploaded by 11:59PM on Wednesdays. Office hrs. Use this sessions for Q\u0026amp;A. @P. Wednesdays 2PM\u0026ndash;4PM, Eng. Building#2 #323 (+ by appointment) @Y. Mondays 1PM\u0026ndash;3PM, Daewoo Hall #534 Schedule (tentative) # W1. Overview, recap, and linear models W2. Approx: Universal approximation with shallow nets W3. Approx: Infinite-width and kernels W4. Approx: Benefits of depth W5. Optim: Convex optimization and generalizations W6. Optim: SGD and flow-based analyses W7. Optim: Implicit bias W8. Gen: Concentration of measures, uniform convergence W9. Gen: Rademacher complexities, covering numbers W10. Gen: Chaining and VC dimensions W11. Recent: Generalization W12. Recent: Approximation W13. Recent: Optimization W14. Recent: Architectures W15. Student Presentations - 1 W16. Student Presentations - 2 Further Readings # Shalev-Shwartz and Ben-David, Understanding Machine Learning: From Theory to Algorithms "},{"id":2,"href":"/docs/teaching/fall24ml/","title":"24F - Intro to ML","section":"Teaching","content":" EECE454: Intro to ML Systems (Fall 2024) # Team # Instructor. Jaeho Lee ì´ì¬í˜¸ âœ‰ï¸ TA. Minjae Park ë°•ë¯¼ì¬ âœ‰ï¸ Location \u0026amp; Time # Class. Mondays/Wednesdays, 09:30\u0026ndash;11:00 @ Eng Bldg 3, Room 115 Office Hr. Wednesdays, 16:00\u0026ndash;17:00 @ Eng Bldg 2, Room 323 Schedule (tentative) # W1. Introduction / Basic Linear Algebra\n(9/2, 9/4) W2. Basic Probability / Supervised Learning and Linear Regression\n(9/9, 9/11) W3. Chuseok holidays\n(9/16, 9/18) W4. Simple Classifiers / Support Vector Machines\n(9/23, 9/25) W5. Kernel SVM / K-Means Clustering\n(9/30, 10/2) W6. Gaussian Mixture Models\n(10/7, 10/9) W7. Dimensionality Reduction\n(10/14, 10/16) Virtual. Decision Tree\n(online) W8. Mid-Term\n(10/21, 10/23) W9. Deep Learning Basics / Backprop\n(10/28, 10/30) W10. Neural network training / Neural network training\n(11/4, 11/6) W11. (Out of town)\n(11/11, 11/13) W12. Vision: Architectures / Vision: Representation Learning\n(11/18, 11/20) W13. Vision: Generative Models / Vision: Diffusion Models\n(11/25, 11/27) W14. Language: Architectures / Language: Representation Learning\n(12/2, 12/4) W15. Multimodal Learning / Efficient ML\n(12/9, 12/11) W16. Final Project Presentation\n(12/16, 12/18) "},{"id":3,"href":"/docs/teaching/fall25dlt/","title":"25F - DL Theory","section":"Teaching","content":" EECE695D: Deep Learning Theory (Fall 2025) # Team # Instructor. Jaeho Lee ì´ì¬í˜¸ âœ‰ï¸ TA. Byung-ki Kwon ê¶Œë³‘ê¸° âœ‰ï¸ Location \u0026amp; Time # Class. Mondays/Wednesdays, 2PM\u0026ndash;3:30PM, EB2, #109 Office Hr. Mondays, 5PM\u0026ndash;6PM, GoAround Coffee @RIST (or by request) Schedule (tentative) # W1. Overview and Basics of Statistical Learning Theory (9/1, 9/3) W2. Approx: Universal approximation with shallow nets (9/8, 9/10) W3. Approx: Infinite-width and kernels (9/15, 9/17-1, 9/17-2) W4. Approx: Benefits of depth\n(9/22-1, 9/22-2, 9/24 ) W7. Optim: Convex optimization and generalizations\n(9/29, 10/1) W6. Chuseok Holidays W8. Optim: SGD and flow-based analyses\n(10/13, 10/15) W9. Mid-Term W10. Optim: Implicit bias (11/3, 11/5) W11. Gen: Concentration of measures, uniform convergence (11/7, 11/9) W12. Gen: Rademacher complexities, covering numbers W13. Gen: Chaining and VC dimensions W14. Recent Results W15. Student Presentations - 1 W16. Student Presentations - 2 Textbook # Main. MJT\u0026rsquo;s lecture notes: old ver. Sub. Shalev-Shwartz and Ben-David, Understanding Machine Learning: From Theory to Algorithms "},{"id":4,"href":"/docs/teaching/fall25ml/","title":"25F - Intro to ML","section":"Teaching","content":" EECE454: Intro to ML Systems (Fall 2025) # Team # Instructor. Jaeho Lee ì´ì¬í˜¸ âœ‰ï¸ TA 1. Minseok Kim ê¹€ë¯¼ì„ âœ‰ï¸ TA 2. Taesun Yeom ì—¼íƒœì„  âœ‰ï¸ Location \u0026amp; Time # Class. Mondays/Wednesdays, 09:30\u0026ndash;11:00 @ LG Hall, #105 Office Hr. Wednesdays, 17:00\u0026ndash;18:00 @ GoAround Coffee (or by request) Schedule (tentative) # W1. Introduction / Basics of ML\n(9/1, 9/3) W2. Linear Regression / Simple Classifiers\n(9/8, 9/10) W3. Support Vector Machines / Kernel SVM\n(9/15, 9/17) W4. K-Means Clustering / Gaussian Mixture Models\n(9/22, 9/24) W5. Dimensionality Reduction / Decision Trees\n(9/29, 10/1) W6. Chuseok Holidays\n(10/6, 10/8) W7. Deep Learning Basics / Backprop\n(10/13, 10/15) W8. Mid-Term\n(10/20, 10/22) W9. Training Neural Networks\n(10/27, 10/29) W10. Bits of Vision: Tasks, Architectures, Representation Learning\n(11/3, 11/5) W11. Visual Generative Models: VAE, GAN, Diffusion\n(11/10, 11/12) W12. Bits of Language: Tasks, Architectures, Representation Learning\n(11/17, 11/19) W13. Large Language Models and Multimodal LLMs\n(11/24, 11/26) W14. Application to Robotics and Reinforcement Learning\n(12/1, 12/3) W15. ML Efficiency / Societal Issues\n(12/8, 12/10) W16. Final Project Presentation\n(12/15, 12/17) "},{"id":5,"href":"/docs/teaching/","title":"Teaching","section":"Docs","content":" 2023 Fall # EECE454 Introduction to Machine Learning Systems\n2023 Spring # EECE695D Deep Learning Theory (comments from students)\n2022 Fall # EECE695D-01 Efficient Machine Learning Systems (comments from students)\n(EduTech Teaching Excellence Award)\nPast Courses # As a TA, I have taught:\nFA17 TA, ECE563@UIUC Information theory. SP17 TA, ECE498@UIUC Introduction to stochastic systems. FA15 TA, ECE598@UIUC Statistical learning theory. "},{"id":6,"href":"/docs/teaching/spring24effml/","title":"24S - Efficient ML","section":"Teaching","content":" EECE695E: Efficient ML Systems (Spring 2024) # Team # Instructor. Jaeho Lee ì´ì¬í˜¸\nfirstname.lastname@postech.ac.kr TA. Hagyeong Lee ì´í•˜ê²½\nfirstnamelastname@postech.ac.kr Location \u0026amp; Time # Class. Mondays \u0026amp; Wednesdays, 11:00AM\u0026ndash;12:15PM, PIAI 122. Office Hr. Mondays 5:00PM\u0026ndash;6:00PM, Eng. Building #407 (+ by appointment). Schedule (tentative) # W1. Deep Learning Recap (2/19) Introduction to Efficient ML \u0026amp; Logistics (2/21) Computations of DL W2. Sparsity (2/26) Algorithmic aspects of pruning (2/28) System aspects of pruning W3. Quantization + Special Lecture (3/4) Basic ideas of quantization (3/6 -\u0026gt; 3/8, Fri) Special Lecture by Tae-ho Kim @ Nota W4. Quantization + Distillation (3/11) PTQ and QAT (3/13) Distillation W5. Neural Architecture Search (3/18) Search Space, Search Strategy (pt 1) (3/20) Search Strategy (pt 2), Performance Evaluation W6. Adaptation (3/25) Continual Learning (3/27) Meta-Learning \u0026amp; Test-time Training W7. Parallelism (4/1) Data and Model Parallelism (4/3) Advanced Topics W8. (Mid-term week \u0026amp; Election Day) (4/8, 4/10) W9. Model Merging \u0026amp; Editing (4/15) Model Merging (4/17) Model Editing W10. Data Efficiency (4/22) Dataset Compression (4/24) Data Compression W11. Topics on LLM - 1 (4/29) Transformers \u0026amp; LLM Basics (5/1) Efficient Attention W12. Topics on LLM - 2 (5/6) Parameter-Efficient Fine-Tuning (5/8) Parameter-Efficient Fine-Tuning W13. Topics on LLM - 3 (5/13) Decoding Strategies (5/15) Buddha\u0026rsquo;s Day W14. Topics on Diffusion Models (5/20) LLM Compression (5/22) Topics in long sequence handling W15. Presentations Week - 1 (5/27, 5/29) W16. Presentations Week - 2 (6/3, 6/5) Recommended Materials # Blog Posts LLM inference performance engineering by Databricks Videos State-space models tutorial by Sasha Rush Trends in deep learning hardware by Bill Dally Related Courses Efficient deep learning systems at HSE university and Yandex Machine learning compilation at CMU TinyML and efficient deep learning at MIT Machine learning hardware and systems at Cornell Advances in Foundation Models at Stanford Books and surveys Algorithms for modern hardware by Sergey Slotin Efficient deep learning book by Menghani and Singh Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities by Bartoldson et al. "},{"id":7,"href":"/docs/teaching/spring25effml/","title":"25S - Efficient ML","section":"Teaching","content":" EECE695E: Efficient ML Systems (Spring 2025) # Team # Instructor. Jaeho Lee âœ‰ï¸ TA. Hyunjong Ok âœ‰ï¸ Location \u0026amp; Time # Class. MW 11:00\u0026ndash;12:15 @ PIAI 122. Office Hr. W 17:00\u0026ndash;18:00 @ Terarosa Coffee (+ by appointment). Schedule (tentative) # W1. Warm-Up (2/17) Intro \u0026amp; Administrivia (2/19) Computations of Deep Learning W2. Sparsity (2/24) Basic ideas (2/26) Advanced: Regularization, Structures, and Systems Presentation List out W3. Quantization (3/3) Data numerics and K-means (3/5) Linear quantiztaion and advanced stuffs Presentation Registration Due W4. NAS \u0026amp; KD (3/10) Knowledge Distillation (3/12) Neural Architecture Search W5. Efficient Training \u0026amp; Tuning (3/17) Continual Learning (3/19) Meta-Learning W6. Efficient Training \u0026amp; Tuning (3/24) Merging and Editing (3/26) Efficient Fine-Tuning W7. (Week off; Instructor at Brussel ğŸ‡§ğŸ‡ª) (3/31, 4/2) Project Proposals Due W8. Parallelism (4/7) Parallelism 1 (4/9) Parallelism 2 W9. Data Efficiency + (4/14) Data Efficiency, Presentation 1, (4/16) Presentation 2, Presentation 3 W10. LLM Compression + (4/21) LLM Compression, Presentation 1 (4/23) Presentation 1, Presentation 2 W11. Long-Context LLMs + (4/28) Long-Context LLMs, Presentation 1 (4/30) Presentation 1, Presentation 2 W12. Low-Precision Training + (5/7) Low-Precision Training, Presentation 1 (5/12) Presentation 1, Presentation 2 W13. Test-Time Scaling + (5/14) Test-time Scaling, Presentation 1 (5/19) Presentation 1, Presentation 2 W14. Efficient Diffusion Model + (5/21) Efficient Diffusion Model, Presentation 1 (5/26) W15. Efficient Neural Rendering + (5/28, 6/2) W16. Poster Session (6/5) Recommended Materials # Blog Posts LLM inference performance engineering by Databricks Videos State-space models tutorial by Sasha Rush Trends in deep learning hardware by Bill Dally Related Courses Efficient deep learning systems at HSE university and Yandex Machine learning compilation at CMU TinyML and efficient deep learning at MIT Machine learning hardware and systems at Cornell Advances in Foundation Models at Stanford Books and surveys Algorithms for modern hardware by Sergey Slotin Efficient deep learning book by Menghani and Singh Compute-Efficient Deep Learning: Algorithmic Trends and Opportunities by Bartoldson et al. "},{"id":8,"href":"/docs/teaching/fall23ml/","title":"23F - Intro to ML","section":"Teaching","content":" EECE454: Introduction to ML Systems (Fall 2023) # Team # Instructor. Jaeho Lee ì´ì¬í˜¸\nfirstname.lastname@postech.ac.kr TA. Minkyu Kim ê¹€ë¯¼ê·œ\nfirstname.lastname@postech.ac.kr Location \u0026amp; Time # Class. Mondays/Wednesdays, 09:30AM\u0026ndash;10:45AM @ 106 LG Hall Office Hr. Mondays, 04:00PM\u0026ndash;05:00PM @ 407 Eng Bldg 2 Schedule (tentative) # W1. Introduction / Basic Linear Algebra\n(9/4, 9/6) W2. Basic Probability / Simple Models\n(9/11, 9/13) W3. Simple Models / Support Vector Machines\n(9/18, 9/20) W4. Kernel SVM / K-Means Clustering\n(9/25, 9/27) W5. Gaussian Mixture Models\n(10/2, 10/4) W6. Decision Tree\n(10/9, 10/11) W7. Dimensionality Reduction\n(10/16, 10/18) W8. Mid-Term\n(10/23, 10/25) W9. Deep Learning Basics / Convolutional Networks\n(10/30, 11/1) W10. Backprop / Neural Network Training\n(11/6, 11/8) W11. Neural Network Training / -\n(11/13, 11/15) W12. Deep ConvNets / Generative Models\n(11/20, 11/22) W13. Generative Models / Language Models\n(11/27, 11/29) W14. Language Models / Multimodal Learning\n(12/4, 12/6) W15. Efficient ML / Deep Learning Theory\n(12/11, 12/13) W16. Final Project Presentation\n(12/18, 12/20) "},{"id":9,"href":"/docs/me/","title":"About me","section":"Docs","content":" Appointments # Current # Assistant Professor EE@POSTECH (adj. AI, DST)\n2022.03\u0026ndash;Present Adjunct Professor, iCreate@Yonsei\n2022.09\u0026ndash;Present Visiting Faculty Researcher, Google\n2023.09\u0026ndash;Present Previous # Postdoctoral Researcher, EE@KAIST (ì „ë¬¸ì—°êµ¬ìš”ì›)\nHost: Jinwoo Shin\n2019.03\u0026ndash;2022.03 ASAN Fellow, Center for Data Analysis @ The Heritage Foundation\nHost: Salim Furth, James Sherk\n2013.05\u0026ndash;2013.07 Education # Ph.D. (2019). Electrical and Computer Engineering\nUniversity of Illinois at Urbana-Champaign\nAdvisor: Maxim Raginsky\nThesis: \u0026ldquo;Robustness and Generalization Guarantees for Statistical Learning of Generative Models\u0026rdquo;\nM.S. (2015). Electrical and Computer Engineering\nUniversity of Illinois at Urbana-Champaign\nAdvisor: Maxim Raginsky\nThesis: \u0026ldquo;MMSE estimation from Quantized Observations in the Nonasymptotic Regime\u0026rdquo;\nB.S. (2013). Electrical Engineering, Management Science (double major)\nKorea Advanced Institute of Science and Technology\nHonor: Summa cum laude\nTalks (selected) # Tutorial, KICS Fall Conference, Gyeongju (2023.11.22)\n\u0026ldquo;Compressing Giant Neural Networks\u0026rdquo; Talk, Optimization \u0026amp; Machine Learning Workshop, UNIST (2023.08.24)\n\u0026ldquo;Problems that Arise in the Optimization of Neural Fields\u0026rdquo; Tutorial, KAIA Summer Conference, Yeosu (2023.07.17)\n\u0026ldquo;Recent Trends in Neural Data Compression: From Generative Models to Neural Fields\u0026rdquo; Tutorial, Korea SatCom Forum, Seoul (2023.05.31)\n\u0026ldquo;Recent Trends in Large Language Models\u0026rdquo; Miscellaneous Writings # Greece: Austerity Doesn\u0026rsquo;t Involve Public-Sector Layoffs\nJaeho Lee\nThe Daily Signal, 2013 (The Orange County Register, 2013)\në‚˜ë§Œì˜ ëª©í‘œ, ë‚˜ë§Œì˜ ê¸°ì¤€\nì´ì¬í˜¸\në™ì•„ì¼ë³´: ë‚´ê°€ ë§Œë‚œ ëª…ë¬¸ì¥, 2022\nì–´ë–¤ ì—°êµ¬ë¥¼ í• ì§€ ëª¨ë¥´ê² ì–´ìš”\nì´ì¬í˜¸\ní¬í•­ê³µëŒ€ì‹ ë¬¸: ë…¸ë²¨ë™ì‚°, 2023\n"},{"id":10,"href":"/docs/teaching/comments/_dlt_2023/","title":"Comments from DLT (2023 Spring)","section":"Teaching","content":"Here are some comments from the students\u0026mdash;I take this comments by heart.\nêµìˆ˜ë‹˜ê»˜ì„œ ìˆ˜ì—… ì¤€ë¹„ë¥¼ ì¶©ì‹¤íˆ í•˜ì‹  ê²ƒì´ ëŠê»´ì§‘ë‹ˆë‹¤. í•œ í•™ê¸° ë™ì•ˆ ìœ ìµí•œ ë‚´ìš©ì„ ë°°ìš¸ ìˆ˜ ìˆëŠ” ì¢‹ì€ ê¸°íšŒì˜€ìŠµë‹ˆë‹¤. ì¢‹ì€ ê°•ì˜ ê°ì‚¬í•©ë‹ˆë‹¤~ ìˆ˜ì—… ë‚´ìš©ì´ ì–´ë ¤ì›Œ ê°€ë” ë”°ë¼ê°€ê¸° ì–´ë ¤ì› ìŒ. ê³¼ì œ ë‚´ìš©ì€ ìˆ˜ì—…ê³¼ ê´€ë ¨ì´ ìˆì§€ë§Œ, ê°œì¸ì ìœ¼ë¡œëŠ” ê°•ì˜ ìŠ¬ë¼ì´ë“œì™€ ìˆ˜ì—… êµì¬ë§Œ ë³´ê³  ê³¼ì œë¥¼ í’€ ìˆ˜ ì—†ì—ˆë˜ ê²½ìš°ê°€ ë§ì•˜ê¸° ë•Œë¬¸ì—, ë‹¤ë¥¸ êµì¬ë“¤ì„ ì°¸ê³ í•˜ê³  ë”°ë¡œ ê³µë¶€í•˜ëŠ” ì‹œê°„ì„ ë§ì´ ê°€ì§ˆ ìˆ˜ ë°–ì— ì—†ì—ˆìŒ. ì´í•´ë§Œ ì˜ í•˜ê³  ì‹œê°„ íˆ¬ìë¥¼ ì¢€ ë” í•œë‹¤ë©´ ì´ë¡ ì ìœ¼ë¡œ ë§ì€ ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŒ. pptê°€ ì¢€ ë” highlevelì—ì„œì˜ íë¦„ì„ ì˜ ì´í•´í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. ì‹ ì „ê°œë¥¼ ì •ì‹  ì—†ì´ ë”°ë¼ì ë‹¤ê°€ ì •ì‘ ì–´ë–¤ ì‹ì„ ì „ê°œí–ˆëŠ”ì§€ ê·¸ ì˜ë¯¸ë¥¼ ì˜ ì´í•´í•˜ì§€ ëª»í•˜ê³  ì§€ë‚˜ê°€ë²„ë¦¬ëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ ì•„ì‰¬ì› ìŠµë‹ˆë‹¤. Jaeho: Thank you for the great suggestion. I will try to add some \u0026ldquo;overview\u0026rdquo; slides in the beginning and the end of all lectures. ì¢‹ì€ ê°•ì˜ë¥¼ í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì—°êµ¬ì— analyzingì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê¸°ì´ˆë¥¼ ë§Œë“¤ì–´ ì£¼ì…¨ìŠµë‹ˆë‹¤! êµìˆ˜ë‹˜ê»˜ì„œ ìˆ˜ì—… ì¤€ë¹„ì— ì‹ ê²½ì“°ì‹ ê²ƒì´ ëŠê»´ì§€ëŠ” ìˆ˜ì—…ì…ë‹ˆë‹¤. êµìˆ˜ë‹˜ê»˜ì„œ ì–´ë ¤ìš´ ë¶„ì•¼ì— ëŒ€í•´ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ê³¼ëª©ì„ êµ¬ì„±í•˜ì‹œê³  ì„¤ëª…ì„ ìì„¸íˆ í•´ì£¼ì…¨ê¸° ë•Œë¬¸ì— ì¢‹ì€ ê³¼ëª©ì´ì—ˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë‹¤ë§Œ, ê°œì¸ì ìœ¼ë¡œëŠ” ë¶„ì•¼ê°€ ì–´ë µê³  ì œê°€ ìµìˆ™í•˜ì§€ ì•Šì€ ì  ë•Œë¬¸ì— ì´í•´ê°€ ì–´ë ¤ìš´ ë¶€ë¶„ì´ ë§ì•˜ë˜ ë¶€ë¶„ì´ ì•„ì‰¬ìš´ ì ì´ì—ˆë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë”¥ëŸ¬ë‹ ë’¤ì— ìˆëŠ” ì´ë¡ ì— ëŒ€í•´ ì•Œê²Œ ë˜ì–´ ì •ë§ ìœ ìµí–ˆìŠµë‹ˆë‹¤. ì •ë§ í¥ë¯¸ë¡œì› ê³  ì´ëŸ° ìˆ˜ì—…ì´ ë§ì´ ìƒê²¼ìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤ ì•„ì£¼ í¥ë¯¸ë¡­ì§€ë§Œ ì–´ë ¤ìš´ ë‚´ìš©ì„ ìµœëŒ€í•œ ì˜ í’€ì–´ë‚´ì–´ ì„¤ëª…í•˜ì‹­ë‹ˆë‹¤. ìˆ˜ì—… ë¡œë“œ ë˜í•œ í•™ìƒì„ ì˜ ë°°ë ¤í•´ ì£¼ì‹­ë‹ˆë‹¤. ìµœê³ ì˜ êµìˆ˜ë‹˜ì´ì‹­ë‹ˆë‹¤. The professor has a very good command in English. The lectures are challenging, but well designed. I do recommend this course for people who are interested in ML/DL or just the theoretical aspect of things. Being able to implement ML/DL algorithm is nice, but having a strong foundation in theoretical background do give you an edge, not only in research but also when working in the industry. í˜•ì‹ì ì¸ ê³¼ëª©ë“¤ì„ ë“¤ì„ë•ŒëŠ” ì‹œê°„ë„ ì•„ê¹ê³  í˜„íƒ€ê°€ ì™”ëŠ”ë°, ì´ ìˆ˜ì—…ì€ ë§¤ìš° ìœ ìµí–ˆê³  ì´ëŸ¬ê¸° ìœ„í•´ ëŒ€í•™ì›ì— ì™”ì§€ë¼ëŠ” ë§Œì¡±ê°ë„ ë“¤ì—ˆìŠµë‹ˆë‹¤. ëª©ì†Œë¦¬ëŠ” ì˜ ì•ˆë“¤ë¦¬ê³  ìˆ˜ì—…ì‹œê°„ì— ì´ì•¼ê¸° í•´ì¤€ ë‚´ìš©ì„ ì´í•´ê°€ ì˜ ì•ˆë˜ì„œ ë©”ì¼ë¡œ ì§ˆë¬¸í•˜ë©´ ìˆ˜ì—…ì—ì„œ ë‹¤ í•œ ì–˜ê¸°ë¼ê³ , ìˆ˜ì—…ì„ ì•ˆë“¤ì—ˆëƒê³  ë§í•˜ë©´ì„œ ê²°êµ­ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì£¼ì§€ì•ŠìŒ. Jaeho: I personally do not recall this specific event, but I do take these comments by heart. Thank you for the feedback\u0026mdash;I will take more care. í•œ í•™ê¸° ë™ì•ˆ ì¢‹ì€ ê°•ì˜ ê°ì‚¬í–ˆìŠµë‹ˆë‹¤. ê²°ì„ ë§ì´ í•´ì„œ ì£„ì†¡í•©ë‹ˆë‹¤ ã…  ì •ë§ ì™„ë²½í•œ ê³¼ëª©ê³¼ ì™„ë²½í•œ êµìˆ˜ë‹˜ì´ì…¨ìŠµë‹ˆë‹¤. ë”± í•œê°€ì§€ ì•„ì‰¬ìš´ ì ì€ ê³¼ì œê°€ ë” ë§ê±°ë‚˜ ì‹œí—˜ì´ ìˆì—ˆìœ¼ë©´ ë” ì¢‹ì•˜ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. êµ‰ì¥íˆ ì–´ë ¤ìš°ë‚˜ ì–»ì„ ì ì´ ìˆìŠµë‹ˆë‹¤. êµìˆ˜ë‹˜ ì–´ë ¤ìš´ë‚´ìš©ì„ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. êµìˆ˜ë‹˜ê»˜ì„œ ê°•ì˜ë¥¼ ë§¤ìš° ì˜í•˜ì‹­ë‹ˆë‹¤. "},{"id":11,"href":"/docs/teaching/comments/_effml_2022/","title":"Comments from Efficient ML (2022 Fall)","section":"Teaching","content":"Here are some comments from the students\u0026mdash;I take this comments by heart.\nê° í† í”½ì— ê´€í•œ ê´€ë ¨ ë…¼ë¬¸ë“¤ì„ ë§¤ë²ˆ ë¸Œë¦¬í•‘ í•´ì£¼ì‹œëŠ” ë°©ì‹ìœ¼ë¡œ ìˆ˜ì—…ì´ ì§„í–‰ë˜ì—ˆëŠ”ë°, ì„¸ë¯¸ë‚˜ ê°™ì€ ëŠë‚Œìœ¼ë¡œ ë§¤ìš° ì˜ ë“¤ì—ˆìŠµë‹ˆë‹¤. ì¢‹ì€ ê°•ì˜ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. efficient machine learningì— ëŒ€í•œ í­ë„“ì€ ì´í•´ë¥¼ í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. í•´ë‹¹ ìˆ˜ì—…ì„ í†µí•´ ì—°êµ¬ì— í° ë„ì›€ì´ ë  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ë˜í•œ reproducibility challenge ê³¼ì œë„ ì‹ ì„ í•˜ì—¬ ì¬ë¯¸ìˆê²Œ í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. êµìˆ˜ë‹˜ì´ ë§¤ìš° ì¶©ì‹¤í•œ ë‚´ìš©ë“¤ë¡œ ì±„ìš´ ìë£Œë¥¼ ì œê³µí•´ ì£¼ì‹  ì ì´ ê°€ì¥ ì¢‹ì•˜ìŒ. êµìˆ˜ë‹˜ê»˜ì„œ ì—´ì •ì ìœ¼ë¡œ í”¼ë“œë°±ì„ ì£¼ì‹œê³ , ê°•ì˜ë¥¼ í•´ì£¼ì‹­ë‹ˆë‹¤. êµìˆ˜ë‹˜ ì‚¬ë‘í•©ë‹ˆë‹¤. ê¸°ëŒ€í–ˆë˜ ê²ƒ ì´ìƒì˜ ìˆ˜ì—…ì´ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ì—…ì˜ ì§ˆë„ ë†’ê³  ë„ì›€ì´ ë§ì´ ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ì¹¨ìˆ˜ì—…ì´ ìœ ì¼í•œ í ì¸ ìˆ˜ì—…ì…ë‹ˆë‹¤. í•œí•™ê¸° ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤! ìˆ˜ì—…ì´ syllabusì— ì íŒëŒ€ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì˜ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤! ì¸ê³µì§€ëŠ¥ ì—°êµ¬ì— ëŒ€í•œ ë‹¤ì–‘í•œ ë¶„ì•¼ë¥¼ ê²½í—˜í•´ë³¼ ìˆ˜ìˆì—ˆìŠµë‹ˆë‹¤. ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•œ ë™í–¥ì„ ì ‘í•  ìˆ˜ ìˆëŠ” ì¢‹ì€ ìˆ˜ì—…ì´ì—ˆìŠµë‹ˆë‹¤. ê°•ì˜ë¥¼ ì—´ì •ì ìœ¼ë¡œ í•˜ì‹­ë‹ˆë‹¤ êµìˆ˜ë‹˜ì´ ë˜ê²Œ ì—´ì •ì ì´ì‹œë‹¤. ìˆ˜ì—… ë‚´ìš©ì´ ë˜ê²Œ íŠ¸ë Œë””í•¨ ì´ë²ˆ í•™ê¸° ë™ì•ˆ ê°ì‚¬í–ˆìŠµë‹ˆë‹¤! í•œ í•™ê¸° ë™ì•ˆ ìˆ˜ì—… ê°ì‚¬í–ˆìŠµë‹ˆë‹¤! ìµœì‹  ë…¼ë¬¸ ê¸°ë°˜ì˜ ìˆ˜ì—…ê³¼ í˜ì´ìŠ¤ë¶ ì¸í„´ í•˜ì…¨ë˜ ì„ ë°°ë‹˜ ê°•ì˜ëŠ” ì´ë¡ ë¿ë§Œ ì•„ë‹ˆë¼ í˜„ì—…ì´ì•¼ê¸°ë„ ë“¤ì„ ìˆ˜ ìˆì–´ì„œ ì •ë§ ì˜ë¯¸ìˆëŠ” ìˆ˜ì—…ì´ì˜€ìŠµë‹ˆë‹¤. "}]